services:
  ollama-service:
    profiles: [ollma-in-docker]
    image: ollama/ollama:latest # a best practice is to use fixed tag for the image
    volumes:
      - ./ollama:/root/.ollama
    ports:
      - 11434:11434
    # Ollama can run with GPU acceleration inside Docker containers for Nvivia GPUs
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: 1
    #          capabilities: [gpu]


  download-llm-data:
    profiles: [ollma-in-docker]
    image: curlimages/curl:8.6.0
    entrypoint: ["curl", "ollama-service:11434/api/pull", "-d", "{\"name\": \"${LLM}\"}"]
    depends_on:
      ollama-service:
        condition: service_started

  my-app:
    profiles: [ollma-in-docker]
    build: 
      context: ./my-app
      dockerfile: Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM=${LLM}
    stdin_open: true # docker run -i
    tty: true        # docker run -t
    depends_on:
      ollama-service:
        condition: service_started
      download-llm-data:
        condition: service_completed_successfully

  my-app-only:
    profiles: [application]
    build: 
      context: ./my-app
      dockerfile: Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - LLM=${LLM}
    stdin_open: true # docker run -i
    tty: true        # docker run -t

#volumes:
#  ollama-data: